{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bVzred2fT-Aw"
   },
   "source": [
    "# EEG Classification task\n",
    " In this notebook we will see how to classify Electroencephalography (EEG) or brain waves by using Deep Learning algorithms. The data set consists of EEG signals of different subjects while sleeping. Particularly, we have to identify 6 different stages of sleepings from these brain waves.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dh-riGEzUCfZ"
   },
   "source": [
    "## Data processing\n",
    "\n",
    "First of all we need to load the data set. For this project we have the possibility to work with two different formats of the same data. [Here](https://drive.google.com/drive/folders/13zpCyXK8pNuuF5Q62N2AbUvu2FuM-0tC?usp=sharing) you can have access to both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xjHO0Z7JUM3H"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "Wb-CXaybUM7x",
    "outputId": "bae8672e-8a62-4bdc-c02a-2c60058a2894"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file_spectogram = open('Data_Spectrograms.pkl', 'rb')\n",
    "data_spectrogram = pickle.load(file_spectogram)\n",
    "file_raw_signals = open('Data_Raw_signals.pkl', 'rb')\n",
    "data_raw_signals = pickle.load(file_raw_signals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N2aJIOeuURXF"
   },
   "source": [
    "After uploading the data. Let's have a look a little bit closer at these data sets. For both, The data-set consists of EEG sequences of 3000-time steps each and coming from two electrode locations on the head (Fpz-Cz and Pz-Oz) sampled at 100 Hz. That means that each sample con- tains two signals of 3000 samples and that those samples correspond to 30 seconds of recording. More in details:\n",
    "\n",
    "\n",
    "\n",
    "*   **Data_Raw_signals.pkl** contains the sequences and the corresponding labels as two array [sequences, labels].\n",
    "*   **Data_Spectrograms.pkl** contains the spectrograms of the sequences and the correspond- ing labels as two array [spectrograms, labels].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H-3-wf4sUXXt"
   },
   "source": [
    "As you can see, we uploaded the data as lists and in each of those we have two arrays. We expected this because of the dataset description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "FM6Bq1VCUNE4",
    "outputId": "e2608356-21d4-4de6-dad8-8594a3255e0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "2\n",
      "<class 'list'>\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(type(data_spectrogram))\n",
    "print(len(data_spectrogram))\n",
    "print(type(data_raw_signals))\n",
    "print(len(data_raw_signals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kRXc9SuTUmp1"
   },
   "source": [
    "Now, we seperate the sequences and spectograms from the respective labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tKsRVpPzUNI5"
   },
   "outputs": [],
   "source": [
    "sequences = data_raw_signals[0]\n",
    "sequences_labels = data_raw_signals[1]\n",
    "spectograms = data_spectrogram[0]\n",
    "spectograms_labels = data_spectrogram[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tTZ1AEXAUrE7"
   },
   "source": [
    "In this way we have a much cleaner idea of our data:\n",
    "\n",
    "\n",
    "\n",
    "*   For both types of data we have 15375 examples, it means that around 15k people took the experiment\n",
    "*   In case of the raw signals, the shape is composed of 2 singals of just 3000 time steps\n",
    "*   In the case of the spectogram the signals are processed and so for each of the two signals the EEG are sampled at 100 Hz for 30 seconds\n",
    "\n",
    "From now on, it is up to us which type of data we want to work with: we can choose either one of them or both. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "rMIw4rsaUvJv",
    "outputId": "274913f4-cd79-4010-fa9b-5b692dabc4fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15375, 2, 3000)\n",
      "(15375,)\n",
      "(15375, 2, 100, 30)\n",
      "(15375,)\n"
     ]
    }
   ],
   "source": [
    "print(sequences.shape)\n",
    "print(sequences_labels.shape)\n",
    "print(spectograms.shape)\n",
    "print(spectograms_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "893nnoomUxnd"
   },
   "source": [
    "According to our task we have to classify six different sleeping stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "JKqLLX5cU0R-",
    "outputId": "9703e63e-8fac-4949-87a6-a9e469bddf34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(len(np.unique(spectograms_labels)))\n",
    "print(len(np.unique(sequences_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o6wIJOyiU4B7"
   },
   "source": [
    "Let's cahange the shape of the sequences and the spectograms. In this way is easier to work with them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cnnZw9ZxU6oJ",
    "outputId": "210fb742-5bad-424d-8fa5-80fe61b8068f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15375, 100, 30, 2)\n"
     ]
    }
   ],
   "source": [
    "spectograms = spectograms.reshape(spectograms.shape[0], 100, 30, 2)\n",
    "print(spectograms.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VEeqnQl3VA-x"
   },
   "source": [
    "Let's save our processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZtTyHtTbVBvO"
   },
   "outputs": [],
   "source": [
    "## SAVE SPECTOGRAM PROCESSED DATA\n",
    "np.save('spectograms.npy', spectograms)\n",
    "np.save(\"spectograms_labels.npy\",spectograms_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xx8OH9r9VGYg"
   },
   "outputs": [],
   "source": [
    "## SAVE SEQUENCES PROCESSED DATA\n",
    "np.save('sequences.npy', sequences)\n",
    "np.save(\"sequences_labels.npy\",sequences_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KcFCEtyLVJTZ"
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mtNu7wuXVQPa"
   },
   "source": [
    "Now you can avoid doing this data processing every time. You just need to upload the processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OCK2znNVVLCT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Mt52lTGxVLG8",
    "outputId": "c321d1b2-aeea-4b4d-8ac7-43ef806461c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15375, 100, 30, 2), (15375,))"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spectograms = np.load(\"spectograms.npy\", allow_pickle = True)\n",
    "spectograms_labels = np.load(\"spectograms_labels.npy\", allow_pickle = True)\n",
    "spectograms.shape,spectograms_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HUFdq7O-VLKI",
    "outputId": "7149c20c-13e7-470b-958c-27cd8c0dbf39"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15375, 3000, 2), (15375,))"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = np.load(\"sequences.npy\", allow_pickle = True)\n",
    "sequences_labels = np.load(\"sequences_labels.npy\", allow_pickle = True)\n",
    "sequences.shape,sequences_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bVwrmhHjVc4i"
   },
   "source": [
    "## CNN 1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model that we are going to try is a CNN1D with the sequences data which have a shape of (3000,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "POTn3vO1VLOO",
    "outputId": "d334f4c9-a3e2-4947-b80b-5a0a18dcb17d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential,Input,Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZIP909XfVgXy"
   },
   "outputs": [],
   "source": [
    "sequences_labels = to_categorical(sequences_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j2XjM6mTVgdc"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(sequences, sequences_labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "JfMS-gL3Vga8",
    "outputId": "7ca06fe7-23e0-4970-fcbc-dff187f23ddf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12300, 3000, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up the batch size, number of epochs and number of classes. As you can see, here we use 1 epoch but is is like this just for the sake of the example. Because we will use earlystopping, I suggest use to put 100 epochos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5QDRTggzVlWb"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 1 #\n",
    "num_classes = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SKz29yCJVlal"
   },
   "outputs": [],
   "source": [
    "fashion_model = Sequential()\n",
    "\n",
    "fashion_model.add(Conv1D(64, kernel_size=3,activation='relu',padding='same',input_shape=(3000,2)))\n",
    "fashion_model.add(BatchNormalization())\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))\n",
    "fashion_model.add(MaxPooling1D(pool_size=2,padding='same'))\n",
    "\n",
    "fashion_model.add(Dropout(0.2))\n",
    "fashion_model.add(Conv1D(128, kernel_size=3, activation='relu',padding='same'))\n",
    "fashion_model.add(BatchNormalization())\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "fashion_model.add(Flatten())\n",
    "fashion_model.add(Dense(64, activation='relu'))\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))           \n",
    "fashion_model.add(Dropout(0.2))\n",
    "fashion_model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vGxc8LDoVsVb"
   },
   "outputs": [],
   "source": [
    "fashion_model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                      optimizer=keras.optimizers.Adam(),\n",
    "                      metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "HIiDoTkiVslx",
    "outputId": "b20581f3-fa86-49b2-b6de-8d87a86d3925"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12300 samples, validate on 3075 samples\n",
      "Epoch 1/1\n",
      "12300/12300 [==============================] - 16s 1ms/step - loss: 2.7693 - acc: 0.2151 - val_loss: 1.9025 - val_acc: 0.1854\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor = \"val_loss\", patience = 10, mode = \"min\", verbose = 1)\n",
    "train = fashion_model.fit(X_train, y_train, batch_size=batch_size,epochs=epochs,verbose=1,callbacks = [es],\n",
    "                                  validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I ran this model with 100 epochs I was able to reach around 60% in validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z6Hxsb2TWmvK"
   },
   "source": [
    "## CNN 1D + LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because LSTM is supposed to work very well with sequential data, here we will try to combine it with a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IOWgkpMEW2sk"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import GRU\n",
    "\n",
    "from keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from keras.layers import Conv1D, MaxPooling1D, Activation, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# add 1-layer cnn\n",
    "model.add(Conv1D(10, kernel_size=3, padding='same', input_shape=(3000,2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(MaxPooling1D(padding='same'))\n",
    "\n",
    "# add 1-layer lstm\n",
    "model.add(LSTM(10, return_sequences=True, stateful=False))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "KryvJun-W2wj",
    "outputId": "ac6b7428-8d79-454e-e631-7634f2eb6db3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12300 samples, validate on 3075 samples\n",
      "Epoch 1/1\n",
      "12300/12300 [==============================] - 309s 25ms/step - loss: 1.4591 - acc: 0.3933 - val_loss: 2.6321 - val_acc: 0.3311\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor = \"val_loss\", patience = 5, mode = \"min\", verbose = 1)\n",
    "train = model.fit(X_train, y_train, batch_size=100,epochs=1,verbose=1,callbacks = [es],\n",
    "                                  validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also in this case when I ran it with 100 epochs, I reached around 50% accuracy in validation. It is definitely worst than the previous CNN 1D. Unfortunately, although there are guidelines about which model to choose and how tune it, there is not a clear answer and so the only solution is to continue trying different algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LGl8vL54XMlj"
   },
   "source": [
    "## 2D CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model tourned out to be the best among all the algortihms I tried with these data. The model you will see here is the result of different days of tuning it: change kernel size, change units, add or remove layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3xtNkZGrXSyB",
    "outputId": "cd4eca39-d1bd-465e-8758-38f169017ee8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15375, 2, 100, 30)\n"
     ]
    }
   ],
   "source": [
    "# back to the original shape\n",
    "spectograms = spectograms.reshape(spectograms.shape[0], 2, 100, 30)\n",
    "print(spectograms.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EdxYRb2_XTA5"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential,Input,Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "yfs97PBRXTTT",
    "outputId": "770528b6-3112-43a2-a06e-6f90bc75ac43"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = to_categorical(spectograms_labels)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MjEkuOFIXTMh"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(spectograms, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "eO4qC4RGXTJr",
    "outputId": "a48b2047-5556-45d9-8763-9815bdd1dc81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12300, 2, 100, 30)\n",
      "(3075, 2, 100, 30)\n",
      "(12300, 6)\n",
      "(3075, 6)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z9qw0LtTXS9n"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 100\n",
    "num_classes = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mKWdbpCDXfbL"
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "\n",
    "  fashion_model = Sequential()\n",
    "  fashion_model.add(Conv2D(64, kernel_size=(4,4),activation='linear',input_shape=(2,100,30),padding='same'))\n",
    "  fashion_model.add(LeakyReLU(alpha=0.1))\n",
    "  fashion_model.add(MaxPooling2D((2, 2),padding='same'))\n",
    "  fashion_model.add(Dropout(0.1))\n",
    "\n",
    "  fashion_model.add(Conv2D(64, (4,4), activation='linear',padding='same'))\n",
    "  fashion_model.add(LeakyReLU(alpha=0.1))\n",
    "  fashion_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "  fashion_model.add(Dropout(0.1))\n",
    "\n",
    "  fashion_model.add(Conv2D(64, (4,4), activation='linear',padding='same'))\n",
    "  fashion_model.add(LeakyReLU(alpha=0.1))\n",
    "  fashion_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "  fashion_model.add(Dropout(0.1))\n",
    "\n",
    "  fashion_model.add(Conv2D(64, (4,4), activation='linear',padding='same'))\n",
    "  fashion_model.add(LeakyReLU(alpha=0.1))\n",
    "  fashion_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "  fashion_model.add(Dropout(0.1))\n",
    "\n",
    "  fashion_model.add(Conv2D(128, (4,4), activation='linear',padding='same'))\n",
    "  fashion_model.add(LeakyReLU(alpha=0.1))\n",
    "  fashion_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "  fashion_model.add(Dropout(0.1))\n",
    "\n",
    "  fashion_model.add(Conv2D(128, (4,4), activation='linear',padding='same'))\n",
    "  fashion_model.add(LeakyReLU(alpha=0.1))\n",
    "  fashion_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "  fashion_model.add(Dropout(0.1))\n",
    "\n",
    "  fashion_model.add(Conv2D(128, (4,4), activation='linear',padding='same'))\n",
    "  fashion_model.add(LeakyReLU(alpha=0.1))                  \n",
    "  fashion_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "  fashion_model.add(Dropout(0.1))\n",
    "\n",
    "  fashion_model.add(Conv2D(128, (4,4), activation='linear',padding='same'))\n",
    "  fashion_model.add(LeakyReLU(alpha=0.1))                  \n",
    "  fashion_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "  fashion_model.add(Dropout(0.1))\n",
    "\n",
    "  fashion_model.add(Conv2D(128, (4,4), activation='linear',padding='same'))\n",
    "  fashion_model.add(LeakyReLU(alpha=0.1))                  \n",
    "  fashion_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "  fashion_model.add(Dropout(0.1))\n",
    "\n",
    "  fashion_model.add(Conv2D(256, (4,4), activation='linear',padding='same'))\n",
    "  fashion_model.add(LeakyReLU(alpha=0.1))                  \n",
    "  fashion_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "  fashion_model.add(Dropout(0.1))\n",
    "\n",
    "  fashion_model.add(Flatten())\n",
    "  fashion_model.add(Dense(128, activation='linear'))\n",
    "  fashion_model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "  fashion_model.add(Dense(num_classes, activation='softmax'))\n",
    "  fashion_model.compile(optimizer=keras.optimizers.Adam(decay = 1e-15), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "  \n",
    "  return fashion_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9i6OnNvzXfqO"
   },
   "outputs": [],
   "source": [
    "fashion_model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 901
    },
    "colab_type": "code",
    "id": "PmWb3GfZXfnB",
    "outputId": "5ea77c3b-749a-4582-840b-c7117a3f1b17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12300 samples, validate on 3075 samples\n",
      "Epoch 1/100\n",
      "12300/12300 [==============================] - 6s 463us/step - loss: 1.4277 - acc: 0.3803 - val_loss: 1.0844 - val_acc: 0.5802\n",
      "Epoch 2/100\n",
      "12300/12300 [==============================] - 4s 309us/step - loss: 1.0323 - acc: 0.5945 - val_loss: 0.9560 - val_acc: 0.6413\n",
      "Epoch 3/100\n",
      "12300/12300 [==============================] - 4s 317us/step - loss: 0.8738 - acc: 0.6580 - val_loss: 0.8401 - val_acc: 0.6715\n",
      "Epoch 4/100\n",
      "12300/12300 [==============================] - 4s 306us/step - loss: 0.7938 - acc: 0.6854 - val_loss: 0.8166 - val_acc: 0.6683\n",
      "Epoch 5/100\n",
      "12300/12300 [==============================] - 4s 313us/step - loss: 0.7096 - acc: 0.7137 - val_loss: 0.7038 - val_acc: 0.7242\n",
      "Epoch 6/100\n",
      "12300/12300 [==============================] - 4s 305us/step - loss: 0.6861 - acc: 0.7368 - val_loss: 0.6484 - val_acc: 0.7600\n",
      "Epoch 7/100\n",
      "12300/12300 [==============================] - 4s 306us/step - loss: 0.6285 - acc: 0.7524 - val_loss: 0.7550 - val_acc: 0.7229\n",
      "Epoch 8/100\n",
      "12300/12300 [==============================] - 4s 309us/step - loss: 0.6531 - acc: 0.7517 - val_loss: 0.6669 - val_acc: 0.7603\n",
      "Epoch 9/100\n",
      "12300/12300 [==============================] - 4s 312us/step - loss: 0.5830 - acc: 0.7799 - val_loss: 0.6046 - val_acc: 0.7772\n",
      "Epoch 10/100\n",
      "12300/12300 [==============================] - 4s 311us/step - loss: 0.5543 - acc: 0.7862 - val_loss: 0.6030 - val_acc: 0.7834\n",
      "Epoch 11/100\n",
      "12300/12300 [==============================] - 4s 313us/step - loss: 0.5207 - acc: 0.7986 - val_loss: 0.5594 - val_acc: 0.7948\n",
      "Epoch 12/100\n",
      "12300/12300 [==============================] - 4s 309us/step - loss: 0.5145 - acc: 0.7988 - val_loss: 0.5835 - val_acc: 0.7925\n",
      "Epoch 13/100\n",
      "12300/12300 [==============================] - 4s 303us/step - loss: 0.4864 - acc: 0.8118 - val_loss: 0.5710 - val_acc: 0.7880\n",
      "Epoch 14/100\n",
      "12300/12300 [==============================] - 4s 305us/step - loss: 0.4930 - acc: 0.8104 - val_loss: 0.5164 - val_acc: 0.7902\n",
      "Epoch 15/100\n",
      "12300/12300 [==============================] - 4s 316us/step - loss: 0.4628 - acc: 0.8201 - val_loss: 0.5074 - val_acc: 0.8107\n",
      "Epoch 16/100\n",
      "12300/12300 [==============================] - 4s 307us/step - loss: 0.4525 - acc: 0.8272 - val_loss: 0.5617 - val_acc: 0.8023\n",
      "Epoch 17/100\n",
      "12300/12300 [==============================] - 4s 307us/step - loss: 0.4353 - acc: 0.8312 - val_loss: 0.5682 - val_acc: 0.7928\n",
      "Epoch 18/100\n",
      "12300/12300 [==============================] - 4s 313us/step - loss: 0.6232 - acc: 0.7950 - val_loss: 0.6756 - val_acc: 0.7782\n",
      "Epoch 19/100\n",
      "12300/12300 [==============================] - 4s 318us/step - loss: 0.5298 - acc: 0.8203 - val_loss: 0.5731 - val_acc: 0.7984\n",
      "Epoch 20/100\n",
      "12300/12300 [==============================] - 4s 308us/step - loss: 0.5361 - acc: 0.8254 - val_loss: 0.6371 - val_acc: 0.7990\n",
      "Epoch 21/100\n",
      "12300/12300 [==============================] - 4s 309us/step - loss: 0.5246 - acc: 0.8254 - val_loss: 0.6076 - val_acc: 0.7870\n",
      "Epoch 22/100\n",
      "12300/12300 [==============================] - 4s 312us/step - loss: 0.5541 - acc: 0.8122 - val_loss: 0.7105 - val_acc: 0.7779\n",
      "Epoch 23/100\n",
      "12300/12300 [==============================] - 4s 313us/step - loss: 0.5940 - acc: 0.8069 - val_loss: 0.6386 - val_acc: 0.7925\n",
      "Epoch 24/100\n",
      "12300/12300 [==============================] - 4s 313us/step - loss: 0.5211 - acc: 0.8263 - val_loss: 0.5662 - val_acc: 0.8176\n",
      "Epoch 25/100\n",
      "12300/12300 [==============================] - 4s 304us/step - loss: 0.5027 - acc: 0.8347 - val_loss: 0.6029 - val_acc: 0.8065\n",
      "Epoch 00025: early stopping\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor = \"val_loss\", patience = 10, mode = \"min\", verbose = 1)\n",
    "fashion_train = fashion_model.fit(X_train, y_train, batch_size=batch_size,epochs=epochs,verbose=1,callbacks=[es],\n",
    "                                  validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time I ran it until early-stopping blocked it. Clearly, here we reached a very high accuracy in validation in comparison with the others models. I couldn't find anything better than this, however I know that with these data, 5% more accuracy in validation data is possible. I will leave this task to you."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "dh-riGEzUCfZ",
    "KcFCEtyLVJTZ",
    "Z6Hxsb2TWmvK",
    "LGl8vL54XMlj"
   ],
   "name": "GitHub_version.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
